\section{TestGuru: Risk and History}

Our work with Ericsson has taken applied problems and developed research-based solutions. To ensure that our results are acted upon, we plan to integrate our results into TestGuru. TestGuru be a web-based tool that will have three parts. \todo{Emad please elaborate}

\textbf{Leveraging test history to determine risky changes:} Shihab~\cite{} has developed a tool, called CommitGuru, that feeds factors extracted from the software changes and files that are part of the change to determine the likelihood of a change introducing a future defect \ie how risky is the change. CommitGuru uses a logistic regression model, that is trained on historical data, to calculate the probabilities of risk for each change. The factors Features include the number of source code lines changes, the expertise of the author, and the diffusion/dispersion of the change. A major drawback of the current implementation of CommitGuru is that it only uses traditional code (e.g., Cyclomatic complexity) and process (e.g., the number of prior changes) metrics to determine the risk of changes, and does not link help with the localization of such changes.

As part of the this proposal, our goal is to add historical test executions and test log information to assess the risk of a change. While some preliminary works have investigated adding these features to statistical fault model\cite{herzig,others}, our research outcome will be an systematic empirical study of multiple projects to determine how effective the use of historical test data is at predicting risky changes. The features extracted from the test data will mainly focus on leveraging the history and the complexity of the tests. For example, if a test always passes, it maybe less important than a test that occasionally fails and these test failures have been associated with defects being filed in the past. We plan to validate this part using standard validation techniques, such as precision and recall. We will also validate the accuracy of the techniques with Ericsson developers to make sure that our techniques are practical.

\textbf{Test prioritization:} Whereas our first part focuses on the using test history to determine which changes are risky, our second goal of TestGuru is to help prioritize tests based on their effectiveness. Much of the test prioritization work \todo{add ciations} has focused on the organization of tests based on coverage.  Shihab et al. have leveraged the development history to prioritize the creation of tests. Similarly, we would like to use the history of how effective a test was in findings real defects (which we call \emph{historical effectiveness of the test}) to prioritize current tests (rather than prioritize the creation of tests). In addition to using the historical effectiveness of the test, we also plan to leverage the \emph{co-failure information of the tests} to dynamically order testing. For example, if test $B$ always fails when test $A$ fails, then we can re-order our tests and push back the execution of running test $B$ if we see that tests $A$ failed. Such a strategy will allow us to execute the most diverse tests early on so that failures can be found earlier. Prior work on random testing \todo{cite} has proposed the idea of running tests in a diverse way, however, the our idea is to use historical knowledge, which in this case is the co-failure distribution of tests, to guide this randomness. 

We plan to validate the effectiveness of the proposed test prioritization using Ericsson's testing data. In particular, we plan run simulations based on the historical test data gathered by Ericsson to determine 1) how many actual defects can we detect using the proposed strategies and 2) how quickly can we detect these tests. We will compare our two strategies with the current strategies used by Ericsson's testing teams to determine the amount of improvement, if any, of the proposed strategies. In order to draw more general conclusions we also plan to replicate our study on a large set of open source projects and contrast our findings with our findings from Ericsson. 

%Instead of predicting change risk, we can use the same predictors to determine which are the most effective tests. This will build on our first milestone that is limited to co-failure distribution and add information such as developer expertise. \todo{Could make this more interesting by suggesting the next test to be run instead of prioritizing upfront} Test re-ordering 

\textbf{Test creation:} Ericsson managers provided examples where there are many tests execution that pass, but customers still find defects. Hence , in addition to the prioritization of existing tests, new tests always need to be written. The reality is that writing tests for all of the new come is practically infeasible. We plan to combine our change risk models, along with the test effectiveness approach to determine areas of the code that are risky and under tested (i.e., need new tests to be written or have current tests re-written).

%Our models will be helpful in indicating the areas of the system that need critically need new test or need tests to be re-written.


