\section{TestGuru: Risk and History}

Our work with Ericsson has taken applied problems and developed research-based solutions. To ensure that our results are acted upon, we plan to integrate our results into TestGuru. TestGuru be a web-based tool that will have three parts, determining risky changes, test prioritization and test creation. We elaborate on each part below.

\textbf{Leveraging test history to determine risky changes:} Shihab~\cite{Rosen2015FSE} has developed a tool, called CommitGuru, that feeds factors extracted from the software changes and files that are part of the change to determine the likelihood of a change introducing a future defect \ie how risky is the change. CommitGuru uses a logistic regression model, that is trained on historical data, to calculate the probabilities of risk for each change. The factors Features include the number of source code lines changes, the expertise of the author, and the diffusion/dispersion of the change. A major drawback of the current implementation of CommitGuru is that it only uses traditional code (e.g., Cyclomatic complexity) and process (e.g., the number of prior changes) metrics to determine the risk of changes, and does not link help with the localization of such changes.

As part of the this proposal, our goal is to add historical test executions and test log information to assess the risk of a change. While some preliminary works have investigated adding these features to statistical fault model\cite{herzig,others}, our research outcome will be an systematic empirical study of multiple projects to determine how effective the use of historical test data is at predicting risky changes. The features extracted from the test data will mainly focus on leveraging the history and the complexity of the tests. For example, if a test always passes, it maybe less important than a test that occasionally fails and these test failures have been associated with defects being filed in the past. We plan to validate this part using standard validation techniques, such as precision and recall. We will also validate the accuracy of the techniques with Ericsson developers to make sure that our techniques are practical.

\textbf{Test prioritization:} Whereas our first part focuses on the using test history to determine which changes are risky, our second goal of TestGuru is to help prioritize tests based on their effectiveness. Much of the test prioritization work has focused on the organization of tests based on coverage (e.g.,~\cite{AggrawalSEN04}).  Shihab et al. have leveraged the development history to prioritize the creation of tests. Similarly, we would like to use the history of how effective a test was in findings real defects (which we call \emph{historical effectiveness of the test}) to prioritize current tests (rather than prioritize the creation of tests). In addition to using the historical effectiveness of the test, we also plan to leverage the \emph{co-failure information of the tests} to dynamically order testing. For example, if test $B$ always fails when test $A$ fails, then we can re-order our tests and push back the execution of running test $B$ if we see that tests $A$ failed. Such a strategy will allow us to execute the most diverse tests early on so that failures can be found earlier. Prior work on random testing \cite{Duran84TSE, Arcuri2012TSE} proposed the idea of running tests in a diverse way, however, the our idea is to use historical knowledge, which in this case is the co-failure distribution of tests, to guide this randomness. 

We plan to validate the effectiveness of the proposed test prioritization using Ericsson's testing data. In particular, we plan run simulations based on the historical test data gathered by Ericsson to determine 1) how many actual defects can we detect using the proposed strategies and 2) how quickly can we detect these tests. We will compare our two strategies with the current strategies used by Ericsson's testing teams to determine the amount of improvement, if any, of the proposed strategies. In order to draw more general conclusions we also plan to replicate our study on a large set of open source projects and contrast our findings with our findings from Ericsson. 

%Instead of predicting change risk, we can use the same predictors to determine which are the most effective tests. This will build on our first milestone that is limited to co-failure distribution and add information such as developer expertise. \todo{Could make this more interesting by suggesting the next test to be run instead of prioritizing upfront} Test re-ordering 

\textbf{Test creation:} Ericsson managers provided examples where there are many tests execution that pass, but customers still find defects. Hence, in addition to the prioritization of existing tests, new tests always need to be written. The reality is that writing tests for all of the new come is practically infeasible. We plan to combine our change risk models, along with the test effectiveness approach to determine areas of the code that are risky and under tested (i.e., need new tests to be written or have current tests re-written). In particular, once our models indicate that a change is risky, we will look into the tests that are written for the code in that change. We will also examine the historical effectiveness of the existing test and determine whether new tests need to be written or existing tests need to be re-written. It is important to note that we plan to flag areas of the code that need tests to be created, however, the actual creation of the tests will need to be performed by the developers or testing teams.

To validate our approach, we plan to run a historical simulation and to examine the effectiveness of the written tests after a delay period, of for example 1 year. In the simulation, we plan to simulate the writing of tests in the past and then examine the their effectiveness in finding real defects. Such a simulation would indicate how well our test strategy is. To complement the simulations, we plan to also examine the effectiveness of any tests created as a result of our proposed test creation strategy after 1 year of the test's creations to see how well the created tests are doing. Such an experiment is critical since it will allow us to gain insights into what works and what does not, as well as, allow us to ask the creators of the tests their rational for the test's creation, why they think it was effective or not effective and how we can enhance our proposed strategy. This is a clear case where the partnership with Ericsson provide invaluable benefit to the reproach and to Ericsson.

%Our models will be helpful in indicating the areas of the system that need critically need new test or need tests to be re-written.


