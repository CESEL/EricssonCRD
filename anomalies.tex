\subsection{Anomalies: Environment vs Product}

Fault localization is a wide research area including debugging techniques and statistical bug models, so in the NSERC Engage work, we focused on the narrow question -- Did that test fail because of a problem in the development environment or product? This problem is unusually difficult at Ericsson because the environment is complex. As a result, there a large team of Ericsson Ottawa testers who examine the failures that occur in the system after the nightly test run. Our goal in this milestone is to help the team differentiate between environment and product faults and to prioritize the failing tests for investigation.

The expected novel contributions in this milestone are twofold. First, most prioritization techniques determine the order of tests to be run, in contrast, we determine the order of tests to be investigated after a failed run. Second, to categorize faults as product or environment, we conduct an anomaly analysis. Our list of anomalies is used each morning at the testing team's 10am meeting to determine which faults each developer will investigate. 

Is a test failure anomalous? The nature of base station tests makes Ericsson's test environment complex and as a result many tests fail sporadically from environmental problems. Our work has focused on determining the test failure distribution for each test. We label tests that fall outside a one-sided 95\% confidence interval to be anomalies. Each test has its own failure distribution. For example, tests that fail regularly because of environmental problems will have to fail at a statistically significant higher level to be flagged as a potentially interesting test failure. In contrast, a test that passes every day for two months and fails on a nightly run, will be flagged as an anomaly. 

The outcome of this work has been very positively received at Ericsson Ottawa. For example, the testing team use the test anomalies as a technique to prioritize failing tests for investigation. Our interactions with these testers has lead to additional enhancements, such as determining the number of times a test has been an anomaly on the scale of a week, month, and for the current release. Proposed work involves conducting user studies of these testers to determine what and how to show them our results and integrate results into TestGuru. 
%Future: we continue to the add datapoints to the TestGuru reports as we get feedback from the test teams. We are also experimenting with co-failure distribution and clustering. We further dicuss this in \todo{Section} 

From a research perspective it is important to understand if this technique can be applied in other settings. We plan to analyze the failure distribution from other Ericsson sites in Sweden. At our suggestion, Herzig, at Microsoft Research is running the anomaly detection technique at Microsoft. His findings indicate that anomalous test failures are almost always environmental faults and that can be safely ignored as they will not usually reappear in a subsequent run.


