A cellular base station connects cell phones to a voice and data network via
LTE (4G) or 3G standards. The software that runs on these base stations
contains not only complex signalling logic with stringent real-time
constraints, but also must be highly reliable, providing safety critical
services, such as 911 calling. At Ericsson Ottawa, testing base station
software is time consuming and involves expensive specialized hardware. For
example, testers may need to simulate cellular devices, such as when a base
station is overwhelmed by requests from cell users at a music concert. In
order to maximize the return on investment of Ericsson's testing efforts, we
plan to use historical artifacts including past code changes, bugs, peer
reviews, test logs, and test runs to create statistical models to (1) evalute
test effectiveness, (2) prioritize tests, and (2) help developers locate the
cause of a failure. The outcome of this work will advance the state-of-the-art
in test prioritization, test log analysis, and fault localization and provide
Ericsson with a more efficient test infrastructure.

\section*{Background: Overview of Research Approach and Data}

The proposed work intersects three areas of software engineering: test
prioritization and effectiveness, test log analysis, and empirical software engineering. 

Regression testing research has three streams of research~\cite{Yoo2012STVR}.
%
%: minimization, selection, and priorization\cite{Yoo2012STVR}.
%
The first, {\it minimization}, involves eliminating tests that are redundant or
of low value. In the literature, the problem has been reduced to one of code
coverage, for example, tests become redundant as the system evolves and more
than one test covers the same control flow. As a result, much of the work in
this area is algorithmic, such as transforming it into a spanning set problem
\cite{Marre2003TSE}, using divide-and-conquer strategies \cite{Chen1996IPL},
and greedy algorithms \cite{Tallam2005SENotes}. 
%
%These algorithms are based on source code coverage, and miss important
%historical information, such as whether a file is at higher risk to failure
%because it has been changed recently.
The second, {\it selection}, uses the same static analysis techniques such as
coverage \cite{Taha1989COMPSAC} and slicing \cite{Jeffrey2006COMPSAC}, but
selects tests that cover source files that are at higher risk because they have
been changed recently \cite{Rothermel1994ICSE}. 
%
The third, {\it prioritization}, orders tests such that expensive, low-value,
or long running tests are run after tests that find faults early.  We
focus on test prioritization because, at Ericsson, once a test fails an
engineer must intervene to discover the fault, so an ordering that makes the
test run fail early is more cost effective.
%Running further tests has little value. 
While early prioritization techniques continued to use coverage measures to
gauge priority, more recent approaches incorporate the faults found in past
test runs \cite{Kim2002ICSE} and change relationships among files
\cite{Sherriff2007ISSRE} to identify high value tests. 
%
\todo{Cite Herzig and others}

\todo{Is there really a fourth type now, which includes MSR and social measures as well?}

We combine test prioritization techniques with empirical software engineering.
Empirical researchers use historical data to create models to help developers,
for example, identify bugs and risky changes \cite{DAmbros2010MSR}, locate
fault introducing changes \cite{Kim2006ASE}, and understand the faults found
during code review \cite{Rigby2014TOSEM}.
%and identify relevant collaborators \cite{Cataldo2006CSCW}.  
Relatively little work has combined test archives with information from other
artifacts, such as bug reports and code reviews. One of the first studies to
look at historical artifacts was done by co-investigator Dr. Shihab, during his
PhD. In this work, he created a statistical model from the source code history
to help developers decide where to write tests for a legacy system at
Blackberry \cite{Shihab2011SPE}.

A major factor limiting research into historically based test prioritization is
information about past runs and links to other development artifacts.  Ericsson
collects massive histories of logs, test runs, and development artifacts, in
part to provide for reproducibility of software and test lineups and auditing.
Much of the current analysis is done manually where developers must sift
through this information to locate the root cause of a failure. We have begun
to automate and guide developers to help with prioritization test and
localizing faults.



